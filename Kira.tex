% This is "sig-alternate.tex" V2.0 May 2012
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

%\documentclass{acm_proc_article-sp}
%
\documentclass[conference]{IEEEtran}
\usepackage{graphicx}
\usepackage{color}
\usepackage{multirow}
\usepackage{listings}
\usepackage{url}

\lstset{language=java, numbers=left, numberstyle=\tiny\color{black}, numbersep=3pt, escapeinside={\$}, basicstyle=\footnotesize\ttfamily, escapeinside={\%*}{*)}}

\newif\ifdraft
\drafttrue
%\draftfalse                                                                              
\ifdraft
\newcommand{\zhaonote}[1]{{\textcolor{cyan}    { ***Zhao:      #1 }}}
\newcommand{\note}[1]{ {\textcolor{red}    {\bf #1 }}}
\newcommand{\franknote}[1]{{\textcolor{green}    { ***Frank:      #1 }}}
\else
\newcommand{\zhaonote}[1]{}
\newcommand{\franknote}[1]{}
\newcommand{\note}[1]{}
\fi

\newcommand{\up}{\vspace*{-1em}}

\begin{document}
%
% --- Author Metadata here ---
%\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Using Big Data Technology for Science Applications: An Astronomy Use Case}

%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\author{
\begin{tabular}{cccc}
{Zhao~Zhang$^{*,\circ}$} & {Kyle~Barbary$^{\circ,\dagger}$} & {Frank~Austin~Nothaft$^{*,\ddagger}$} & {Evan~Sparks$^*$} \\
{Oliver~Zahn$^\dagger$} & {Michael~J.~ Franklin$^{*,\circ}$} & {David~A.~Patterson$^{*,\ddagger}$} & {Saul~Perlmutter$^{\circ,\dagger}$}
\end{tabular}
\\%\and % use '\and' if you need 'another row' of author names
\begin{tabular}{c}
$^*$ AMPLab, University of California, Berkeley \\
$^\circ$ Berkeley Institute for Data Science, University of California, Berkeley\\
$^\dagger$ Berkeley Center for Cosmological Physics, University of California, Berkeley\\
$^\ddagger$ ASPIRE Lab, University of California, Berkeley \\
\up\up
\end{tabular}
} 

%\numberofauthors{6} 
%\author{
% 1st. author
%\alignauthor Zhao~Zhang\\\
%       \affaddr{AMPLab \& BIDS\titlenote{Berkeley Institute for Data Science}}\\
%       \affaddr{University of California, Berkeley} \\
%       \email{zhangzhao@berkeley.edu}
% 2nd. author
%\alignauthor Kyle~Barbary\\
%       \affaddr{BCCP\titlenote{Berkeley Center for Cosmological Physics} \& BIDS}\\
%       \affaddr{University of California, Berkeley}\\
%       \email{kbarbary@berkeley.edu}
% 3rd. author
%\alignauthor Frank~Austin~Nothaft\\
%       \affaddr{AMPLab \& ASPIRE Lab}\\
%       \affaddr{University of California, Berkeley}\\
%       \email{fnothaft@berkeley.edu}
%\vspace{.2cm} \and         
% 4th. author       
%\alignauthor Evan~Sparks\\\
%       \affaddr{AMPLab}\\
%       \affaddr{University of California, Berkeley}\\
%       \email{sparks@berkeley.edu}       
% 5th. author
%\alignauthor Oliver~Zahn\\
%       \affaddr{BCCP \& BIDS}\\
%       \affaddr{University of California, Berkeley}\\
%       \email{zahn@berkeley.edu} 
% 6th. author
%\alignauthor Michael~J.~Franklin\\
%       \affaddr{AMPLab \& BIDS}\\
%       \affaddr{University of California, Berkeley}\\
%       \email{franklin@berkeley.edu}
%\vspace{.2cm} \and       
% 7th. author
%\alignauthor David~A.~Patterson\\
%       \affaddr{AMPLab \& ASPIRE Lab}\\
%       \affaddr{University of California, Berkeley}\\
%       \email{pattrsn@cs.berkeley.edu}  
% 8th. author
%\alignauthor Saul~Perlmutter\\
%       \affaddr{BCCP \& BIDS}\\
%       \affaddr{University of California, Berkeley}\\
%       \email{perlmutter@berkeley.edu }
%}

\maketitle

\begin{abstract}
Many scientific applications are constructed by composing multiple single-process programs into a dataflow (referred as many-task applications).
Practitioners often use a traditional HPC platform and its software stack to parallelize these analyses.
In this work, we investigate an alternate approach, leveraging the modern big data platform, for this type of application.
We build Kira, a flexible and distributed astronomy image processing toolkit using Apache Spark. 
We also implement a Source Extractor application ~(Kira SE) with the Kira API.
With Kira SE as the use case, we study the programming flexibility, dataflow richness, scheduling capacity and performance
of Apache Spark running on the EC2 cloud.
In terms of performance, by exploiting data locality, Kira SE achieves a 3.7$\times$ speedup over an equivalent C program when analyzing a 1TB
dataset with 512 cores on the Amazon EC2 cloud. 
Leveraging software originally designed for big data infrastructure, Kira SE achieves competitive performance as that of  the C implementation
running on the NERSC Edison supercomputer.
Our experience with Kira indicates that emerging Big Data platforms such as Apache Spark are an efficient alternative for many-task scientific applications.  
\end{abstract}

% A category with the (minimum) three required fields
%\category{D.1.3}{Software}{Programming Techniques}[Distributed programming]
%A category including the fourth, optional field follows...
%\category{J.2}{Computer Applications}{Physical Sciences and Engineering}[Astronomy]

%\keywords{ACM proceedings, \LaTeX, text tagging}

\section{Introduction}
Many scientific discoveries are made through the collection and analysis of large datasets.
Dramatic increases in dataset sizes have made data processing a major bottleneck for scientific research in many disciplines, such as astronomy, genomics, the social sciences, and neuroscience.
Research groups frequently start with a C or Fortran program that is optimized for processing a small amount of data on a single-node workstation and then use distributed processing frameworks to improve processing capacity. Examples include: the Montage astronomy image mosaic application~\cite{jacob09},  the sequence alignment tool BLAST~\cite{altschul90}, and high energy physics histogram analysis~\cite{ekanayake08}.
These applications are known as many-task applications~\cite{raicu08} because they comprise many small single-process tasks that are connected by dataflow patterns.

Scientists have used dedicated workflow systems (e.g. HTCondor~\cite{litzkow88}), parallel frameworks (e.g. Message Passing Interface~\cite{gropp96}), and more recently the data processing system Hadoop~\cite{HADOOP}  to build these applications. Each of these approaches has its own advantages such as provenance tracking, high scalability, and automated parallelism. However, these approaches also have shortcomings such as limited programming flexibility, lack of fault-tolerance, or strict programming model.


Apache Spark~\cite{zaharia12} emerged as a dataflow-based distributed computing system
to answer demands for a tool that could run fast iterative data analyses on very large datasets.
Spark parallelizes work using a directed, acyclic graph~(DAG) based model,
provides resilience against transient failures by tracking computational lineage
and optimizes for data locality when scheduling work.
These features make Apache Spark a compelling candidate distributed platform for enabling many-task applications.

In this work, we investigate the idea of leveraging Apache Spark , as an example of the modern big data ecosystem for many-task applications.
In particular, we build Kira, an astronomy image processing toolkit.
We use a real world application, Source Extractor (Kira SE), to examine the programming flexibility, dataflow richness,
and scheduling capacity of Spark and its surrounding ecosystem. 
We quantitatively evaluate Kira SE's performance to understand the pros and cons of this idea by comparing against
an equivalent C implementation that is parallelized by HPC ecosystem.

%From this study, we propose using a library interface to talk between legacy C/Fortran computational code and a commodity data/control flow system that is backed
%by a distributed file system. We use a real world astronomy application, Source Extractor, to understand the performance of the Spark approach
%by comparing it against a parallel C version running on a shared file system.

Spark can integrate existing astronomy software at a library level so that astronomers have the flexibility 
to reuse the existing software libraries to build new analysis pipeline.
Spark supports a broad range of dataflow patterns such as pipeline, broadcast, scatter, gather, reduce, allgather, 
and alltoall (shuffle). The broad dataflow pattern support can be used to optimize the data transfer between computation stages.
Spark's underlying file system support allows Kira to process data stored on the big data storage system, e.g. HDFS\cite{shvachko10}, 
as well as HPC shared file system, e.g. GlusterFS~\cite{davies13} and Lustre~\cite{donovan03}.
The Kira toolkit also inherits Spark's fault tolerance mechanism, which is a missing feature in the HPC MPI programming model~\cite{gropp96}.
Our experiments show that Spark is capable of managing $O(10^6)$ tasks and that Kira SE runs 3.7x faster than the equivalent C program on shared file system running in the Amazon EC2 cloud on the 1TB Data Release 7 from the Sloan Digital Sky Survey~\cite{york00}. 
We also show that running Kira SE in the Amazon EC2 cloud can achieve competitive performance than that of running the equivalent C program on the NERSC Edison supercomputer. 

Our experience with Kira indicates that emerging Big Data platforms such as Apache Spark are an efficient alternative for many-task scientific applications.   
We believe this is important, because leveraging such platforms would enable scientists to benefit from the rapid pace of innovation and large range of systems and technologies that are being driven by wide-spread interest in Big Data analytics.
Kira is open source software released under an MIT license and is available from \linebreak \url{https://github.com/BIDS/Kira}.


%\franknote{This is a personal preference, but I don't generally think that providing an ``outline'' of the paper is useful, as long as your paper is well organized.}
%\zhaonote{Sure, feel free to remove it, you already did.}

\section{Background}
\label{sec:Background}

This section reviews the science behind sky surveys, introduces the source extraction operation, examines engineering requirements, and discusses the origin and usage of Spark.

\subsection{Sky Surveys}

Modern astronomical research is increasingly based around large-scale sky surveys.
Rather than selecting specific targets of study, such a survey will uniformly observe large
swaths of the sky. Example surveys include the Sloan Digital Sky Survey (SDSS)~\cite{york00},
the Dark Energy Survey (DES)~\cite{dark05}, and the Large Synoptic Survey Telescope~(LSST,
\cite{ivezic08}). Enabled by new telescopes and cameras with wide fields of view, these
surveys deliver huge datasets that can be used for many different scientific studies
simultaneously.

In addition to studying the astrophysical properties of many different individual galaxies,
the large scale of these surveys allows scientists to use the distribution of galaxies to
study dark matter, dark energy, and the properties of gravity---currently the
biggest mysteries in cosmology. These surveys also include a time component: each patch of the sky is imaged many times,
with observations spread over hours, days, weeks or months. With this repeated imaging,
transient events can be detected via ``difference imaging''. Transients such as supernovae
can be detected in large numbers to better measure dark energy, and the large survey area
often results in the discovery of new, extremely rare, transient phenomena.

\subsection{Source Extraction}

Source extraction is a key step in astronomical image processing pipelines.
SExtractor~\cite{bertin96} is a widely used C application for source extraction.
Users need to specify $\sim$40 parameters to run the application. Although
SExtractor is implemented as a monolithic C program for extracting astronomical
objects from images, the application's logic can be further divided as a procedure 
of background estimation, background removal, object detection, and astrometric 
and photometric estimation.

To improve extraction accuracy, atronomers must run multiple iterations of source
extraction by removing the detected objects at the end of each stage. While
the original C program contains the required functionalities for building this iterative process,
it does not expose the interfaces through the command line. To resolve this issue,
SEP~\cite{barbary2015} reorganizes the code base of SExtractor to expose core
functionalities via a library. SEP provides both C and Python interfaces. Users can then
build the iterative process with SEP's primitives. Kira SE is implemented by calling into
the SEP library.

\subsection{Engineering Requirements}
\label{sec:Background-EngReq}

In some observations, such as looking for supernovae explosions, it is important to process
the images in a timely manner. A rapid processing pipeline enables astronomers to trigger follow-up
observations with more sensitive instrumentation before, say, the peak of the supernovae occurs.
High throughput is also needed in large scale
sky survey pipelines that perform real time data analysis, e.g., LSST~\cite{ivezic08}.
The LSST uses a 2.4m-wide optical telescope that captures 3.2 billion pixels per
image. This telescope produces approximately 12.8~GB in 39~seconds
for a sustained rate of $\sim$330~MB per second. A typical night produces 13~TB of data. 
Over the planned 10-year operation, the survey will produce 60~PB of raw data, which will be consolidated into a 15~PB catalog.
This timely processing requirement and the massive amount of data place a performance challenge for the 
processing pipeline. 

\subsection{Spark}
Apache Spark is a dataflow-based execution system that provides a functional, collection
oriented API~\cite{zaharia12}. Spark's development was motivated by a need for a
system that could rapidly execute iterative workloads on very large datasets. Spark has
become widely adopted in industry, and academic research groups have used Spark
for the analysis of scientific datasets in areas such as neuroscience~\cite{freeman14} and genomics~\cite{nothaft15}.

Spark is centered around the Resilient Distributed Dataset~(RDD) abstraction~\cite{zaharia12}.
To a programmer, RDDs appear as an immutable collection of independent items that are 
distributed across the cluster. RDDs are logically immutable and are transformed using a
functional API. Operations on RDDs are evaluated lazily, enabling the system to schedule 
execution and data movement with better knowledge of the operations to be performed than 
systems that immediately execute each stage.

Spark provides Scala, Java, and Python programming interfaces in a functional
programming paradigm. Spark uses HDFS~\cite{shvachko10} for persistent storage, but
it can process data stored in S3 or a shared file system such as
GlusterFS~\cite{davies13} or Lustre~\cite{donovan03}. Spark tracks job lineage, which is used for the
recomputation of data when a node fails. 
%Spark has been extended to support a variety of
%application specific workloads through the GraphX~\cite{gonzalez14} library for graph
%processing, the MLlib~\cite{sparks13} library for machine learning, and SparkSQL~\cite{armbrust15} for relational processing.

\section{Applying Spark to \\ Many-Task Applications}
\label{sec:Capability}

Scientific analysis pipelines are frequently assembled by building a dataflow out of many
single-process programs. Example applications are Montage~\cite{jacob09}, BLAST~\cite{altschul90}, and the high energy physics histogram analysis application~\cite{ekanayake08}. Many-task applications arise in scientific research
domains including astronomy, biochemistry, bioinformatics, psychology, economics, climate science,
and neuroscience. The tasks in these pipelines are typically short lived and are connected by
producer-consumer data sharing relationships. A previous survey study~\cite{katz11} identified
seven common dataflow patterns among a group of many-task applications. The patterns
include \textbf{pipeline}, \textbf{broadcast}, \textbf{scatter}, \textbf{gather},
\textbf{reduce}, \textbf{allgather}, and \textbf{alltoall}. All many-task applications
can be viewed as stages of independent tasks that are linked by these dataflow patterns.

The map-reduce~\cite{dean04} model provides a similar decomposition. Traditional map-reduce
systems as such as Google's MapReduce~\cite{dean04} and Apache Hadoop MapReduce~\cite{HADOOP} abstract
producer-consumer relationships into a map stage and a reduce stage. These two stages are then
linked by a data shuffle. Although these systems have proved very powerful for processing very
large datasets, the map-reduce API has consistently been criticized as inflexible~\cite{dewitt08}.
Additionally, since jobs are restricted to a single map and reduce phase each, tools such as
FlumeJava~\cite{chambers10} are necessary for assembling pipelines of map-reduce jobs. Since
data is spilled to disk at the end of each map and reduce phase, map-reduce performs poorly
on iterative jobs (i.e., jobs that demonstrate the \textbf{pipeline} dataflow pattern)~\cite{zaharia12}.

To resolve these problems, second-generation map-reduce execution systems such as
DryadLINQ~\cite{yu08} and Spark~\cite{zaharia12} allow for applications to be decomposed into
DAGs. In these DAGs, nodes represent computation, and the nodes are
linked by dataflows. In Spark, this decomposition is implemented via the Resilient Distributed Dataset~(RDD)
abstraction~\cite{zaharia12}.

All seven common many-task application dataflow patterns map to
transformations in \linebreak Apache Spark. Table~\ref{tb:Patterns} demonstrates the mapping between
dataflow primitives and Spark operations.

\begin{table}[h]
  \begin{center}
  \caption{Dataflow Pattern Primitives in Spark}
    \begin{small}
    \begin{tabular}{ | p{1.8cm} | p{5.5cm} |}
    \hline
    Pattern & Spark primitive \\
    \hline \hline
    Pipeline & RDD.map()  \\ 
    Broadcast & sparkContext.broadcast() \\   
    Scatter & sparkContext.parallelize() \\ 
    Gather & RDD.collect() \\ 
    Reduce & RDD.reduce() \\ 
    Allgather & RDD.collect().broadcast() \\ 
    Alltoall & RDD.reduceByKey() or \\
 & RDD.repartition() \\ 
    \hline
    \end{tabular}
    \end{small}   
  \label{tb:Patterns}     	
  \end{center}
\end{table}

\up
Beyond using a DAG to describe a computational schedule, Spark improves upon
Hadoop MapReduce by adding support for iterative computation. This is accomplished by using an
in-memory caching model. As compared to other DAG based methods such as DryadLINQ, this enables
the efficient execution of chained \textbf{pipeline} stages. In a chained \textbf{pipeline},
I/O and communication are only performed before the first stage of the chain, and after the
last stage of the chain. More explicitly, Spark only performs a data shuffle whenever a
\textbf{gather}, \textbf{allgather}, or \textbf{alltoall} dataflow is encountered~\cite{zaharia12}.

Spark provides fault tolerance via
lineage-based recomputation. If a partition of data is lost, Spark can recover the data
by re-executing the section of the DAG that computed the lost partition~\cite{zaharia12}. 

\section{Kira Design and Implementation}
\label{sec:Design}

When designing Kira, we focused on improving computation and  I/O cost as well as the expressive programming interface.
Once the design was complete, we tuned Spark's parallelism level and scheduling algorithm for better performance.
Figure~\ref{fig:architecture} shows Kira's overall architecture and the interactions between parts of the design.

\begin{figure}[t]
	\begin{center}
		\includegraphics[width=85mm]{pictures/Kira-Architecture}
		\caption{Overview of Kira's Architecture and Inter-Component Interactions}
		\label{fig:architecture}
		\up\up
  	\end{center}
\end{figure}

To run Kira, we dispatch the application to the Kira Master, which runs on top of Spark Master. 
The Kira Master node is then responsible for managing control flow, dataflow, and task scheduling 
by coordinating the Kira Worker, which runs on top of Spark Worker. 
The Kira Master accesses distributed/parallel file systems for metadata.
The actual I/O is distributed across the Kira Worker nodes in
an embarrassingly parallel operation. When a task is scheduled on a Kira Worker node, the Kira Worker executes
the task by calling out via the Java Native Interface (JNI) to the SEP library that is distributed to all worker nodes.

\subsection{Computation}

We considered three approaches when implementing the Source Extractor algorithm in Kira:

\begin{itemize}
\item We can \textbf{rewrite} the Source Extractor algorithm from scratch,
\item We can connect existing programs into a \textbf{workflow}, or
\item We can reorganize the C-based SExtractor implementation to expose a programmable
\textbf{library} that we call.
\end{itemize}

Although {\em Rewriting}  all functions in the C SExtractor code with Scala seems straightforward to make use of Spark,
there is going to be performance degradation for the computation part of the C SExtractor. 
Additionaly, this approach requires a tremendous amount of effort. 
The \emph{workflow} commonly does not modify the original executable. Spark's
Resilient Distributed Dataset~(RDD) API supports a pipe function, which enables piping data
stored in an RDD to a shell command via standard input and output. It is possible for Kira to
integrate with the C code base at this level. However, this approach sacrifices programming
flexibility in cases where the original application has hard coded program logic that restricts
the implementation of an advanced function. In some cases, these advanced functions can be composed
by iteratively calling only parts of the hard-coded program logic. We demonstrate an example of this in~\S\ref{sec:Programming}.

As a contrast, we propose the \emph{library} approach. This approach inherits the legacy code
base without losing programming flexibility. A use case demonstration  of flexibility is
given in Listing~\ref{lst:SE-Iter}. 
%An additional benefit of integrating at the library
%level is that the data structures are loaded into Spark, which allows users to make use of
%Spark based libraries, such as the machine learning algorithms available through
%MLlib~\cite{sparks13}.

\subsection{Flow Management}

Control flow management is done through Scala for Spark.
Spark supports a rich set of dataflows, as demonstrated in
\S~\ref{sec:Capability}. To construct dataflows in Kira, users need to understand
the SparkContext and RDD APIs.

\subsection{I/O}
\label{sec:Design-I/O}
The Flexible Image Transport System (FITS)~\cite{wells81} format is a widely adopted file format for astronomy
images. Each FITS file combines a ASCII text part for metadata and a binary part for the actual image.
Many sky surveys release data products in the FITS format. This requires Kira has to be compatible with 
the FITS format. 
One the other hand, one notable difference between HDFS and the traditional POSIX compatible file system is 
the exposure of file locality information. HDFS client exposes a set of APIs for users to customized the input file
format while the underlying system guarantees the locality when loading the files. 
Spark in turn calls to the HDFS client APIs to load files from HDFS with locality optimization. 

The mixed text and binary contents of FITS format makes it challenging for Kira to be compatible with.
One way for Kira to be compatible with the FITS format while preserving the file locality is to customize Spark's
HDFS client by passing the FITS class definition. This approach requires changes to the Spark code base and 
introduces additional overhead to manage an extra Spark branch that is compatible with the FITS format.

To better isolate Kira code from Spark, we use an alternate solution that is based on 
the \texttt{SparkContext.binaryFiles()} API, with which we do not have to change the Spark source code.
This API loads all files within a directory as a sequence of tuples. Each tuple contains the file object and a byte
stream containing the contents of the file. We then use jFITS~\cite{jfits} library to convert these byte streams 
into FITS objects that users can transform and compute upon. 


\subsection{Parallelism}

Spark provides parallelism at both the thread and process level. By default, Spark makes use of
thread-level parallelism by launching a single Java Virtual Machine~(JVM) per worker machine.
Users then specify the number of threads to launch per worker (typically, one thread per core).
However, in Kira SE, neither the jFITS nor the SEP libraries are thread safe. To work around this,
we configured Spark to support process level parallelism by launching a worker instance for each
core. This configuration may reduce scalability,
as it increases the number of workers the master manages. However, our experiments with 512
workers in \S~\ref{sec:Performance} show that Kira's scalability is not severely impacted by
worker management overhead.

\subsection{Scheduling}

Spark's task-scheduling policy aims to maximize data locality by using
delay scheduling~\cite{zaharia10ds}. In this scheduling paradigm, if node $n$ has the data needed
to run job $j$, job $j$ will execute on node $n$ if job $j$ would wait less than a threshold time
$t$ to start. The policy is tunable through three options:

\begin{itemize}
\item{spark.locality.wait.process}
\item{spark.locality.wait.node}
\item{spark.locality.wait.rack}
\end{itemize}

These parameters allow users to specify how much time a task will wait before being sent to another
process, node, or rack. For Kira SE, we have found that data balancing can 
impact task distribution, leading to node starvation and a reduction in overall performance.
Loading a 65GB (11,150 files) dataset from SDSS Data Release 2 to a 16-node HDFS deployment ideally should result in 699 files on each node.
In reality, the number of files on each node is varies between 617 and 715.
Enforcing locality with longer {\em spark.locality.wait} time (3000 ms) leads to task distribution imbalance, which makes
Kira SE 4.5\% slower than running with {\em spark.locality.wait} set to 0 ms.
In practice, we set all {\em spark.locality.wait} parameters to zero, so that tasks do not wait for locality.
This setting effectively avoids the starving situation and improves the overall time-to-solution.

The root cause of the delay scheduling ineffectiveness is the small input file size of the Kira SE tasks. 
Each input file is $\sim$6MB, while a typical Spark/Hadoop application accesses files with a block size of 64/128 MB~\cite{shvachko10}.
Delay scheduling works effectively when it is expensive to move the data through network. The small input
file size means that this assumption is not true, leading to reduced cost for scheduling without locality.
We notice that the small file size is why Kira SE is CPU bound, making Kira SE
unable to benefit from faster storage devices. 
We are working on I/O performance to enable Kira to better utilize storage devices, as previously stated in~\S\ref{sec:Design-I/O}.
 
\section{Programming Kira}
\label{sec:Programming}
Kira implements an API that covers three computational problems: Background, Extractor and Ellipse. This API is described
in Table~\ref{tb:Primitives}. ``Background'' methods are used to estimate and remove the image background. The Extractor
API is used for extracting objects and estimating astrometric and photometric parameters. The Ellipse API offers helper
functions for converting between ellipse representations, and for generating masks that are based on an object's elliptical
shape. The \texttt{sum\_circle()}, \texttt{sum\_ellipse()}, and \texttt{kron\_radius()} methods in the extractor category and all
methods in the ellipse category perform batch processing, where the input coordinates are passed as a three dimensional array. By processing objects
in batches, we are able to amortize the cost of each Java Native Interface (JNI) call over many objects.

\begin{table*}[t]
\begin{center}
\caption{Kira Primitives and Explanation}
\label{tb:Primitives}
\begin{tabular}{ |l|l|l| }
\hline
Group & API & Explanation \\ \hline \hline
\multirow{3}{*}{Background} & makeback() & Builds background from an input image \\
 & backarray() & Returns the background as a 2D array \\
 & subbackarray() & Subtracts a given background from image \\ \hline
\multirow{4}{*}{Extractor} & extract() & Returns objects extracted from the input image \\
 & sum\_circle() & Sums data in circular apertures \\
 & sum\_ellipse() & Sums data in elliptical apertures \\ 
 & kron\_radius() & Calculate iron radius within an ellipse \\ \hline
\multirow{3}{*}{Ellipse} & ellipse\_coeffs() & Converts from ellipse axes and angle to coefficient representations \\
 & ellipse\_axes() & Converts from coefficient representations to ellipse axes and angles \\ 
 & mask\_ellipse() & Masks out certain pixels that fall in a given ellipse \\ \hline
\end{tabular}
\up
\end{center}
\end{table*}

This API allows us to build a source extractor in Kira that is equivalent to the SEP
extractor~\cite{barbary2015}.
Listing~\ref{lst:SE} contains pseudocode describing how to implement a source extractor
using Kira's API. This code uses Spark's \texttt{binaryFiles()} method to load input
files from persistent storage. We then map over each file to convert the FITS data into
matrices with associated metadata. In the final map stage, we estimates and remove the
background from the matrix. Once the background is removed, we then extract the objects
from the matrix.

\begin{lstlisting}[caption=Objects Extraction Logic, label=lst:SE, linewidth=0.5\textwidth, xleftmargin=2.5ex]
val input_rdd = sparkContext.binaryFiles(src)
val mtx_rdd = input_rdd.map(f => load(f))
val objects_rdd = mtx_rdd.map(m => {
  /* mask is a 2-d array with 
   * the same dimensions as m
   */
  val mask = null
  val bkg = new Background(m, mask)
  val matrix = bkg.subfrom(m)
  val ex = new Extractor
  val objects = ex.extract(matrix))
})
\end{lstlisting}

In Listing~\ref{lst:SE-Iter}, we demonstrate how the Kira API can be used to perform
iterative image refinement. Although this is a conceptual goal of the original SExtractor
application~\cite{bertin96}, SExtractor does not implement this feature. However, since Kira provides
library level bindings, it is easy to implement a multi-stage refinement pipeline.

\begin{lstlisting}[caption=Iterative Objects Extraction Logic, label=lst:SE-Iter, linewidth=0.5\textwidth, xleftmargin=2.5ex]
val input_rdd = sparkContext.binaryFiles(src)
val mtx_rdd = input_rdd.map(f=>load(f))
val objects_rdd = mtx_rdd.(m => {
  /*mask is a 2-d array with 
   *the same size of m
   */
  var mask = null
  var ex = new Extractor   
  for(i <- 0 until 5) {
    var bkg = new Background(m, mask)
    var matrix = bkg.subfrom(m) 
    var objects = ex.extract(matrix)
    mask = mask_ellipse(objects)  
  }
  objects
})
\end{lstlisting}

Listing~\ref{lst:SE-Iter} is similar to Listing~\ref{lst:SE}. However, we wrap the
extraction logic in a for loop. At each stage, we make an update to the mask variable, which
is then used for further refinement in the next loop iteration.

\section{A Solution with HPC Ecosystem}
A typical way of parallelize many-task application such as source extractor is to use scripting language
or MPI to launch multiple tasks concurrently. Figure~\ref{fig:hpc-architecture} shows architecture and
inter-component interactions of such a solution.

\begin{figure}[t]
	\begin{center}
		\includegraphics[width=85mm]{pictures/HPC-Architecture}
		\caption{Overview of a Parallel Version of Source Extractor Architecture and Inter-Component Interactions with HPC Ecosystem}
		\label{fig:hpc-architecture}
		\up\up
  	\end{center}
\end{figure}

All the input files are stored in the shared file system, e.g. GlusterFS or Luster. We use a master script
first to read all the input file names, then partition the file names into partitions. After that, the master
script informs the worker scripts on each node to process and independent partition of files in parallel 
in a batch manner.

For the GlusterFS, all metadata is spread across all nodes. So the metadata query from the master script
needs to communicate with all nodes. And the actual file I/O are done within the worker scripts, while 
this POSIX I/O does not have the notion of locality. For the case of Luster file system, the metadata is
stored on one or a few metadata servers, while the actual file I/O is handled in a similar way as GlusterFS.




\section{Performance}
\label{sec:Performance}

We evaluate Kira SE's performance against the C implementation with the SEP library
(referred as the C version in the following text). Because it uses SEP, Kira SE performs an identical amount of
computation and I/O as the C version.

To understand Spark's overhead, we compare the C implementation against Kira SE on a single
machine. Then we fix the problem size and scale Kira SE and the C implementation across a
varying number of machines on EC2 to understand the relative scalability of each approach.
The 1TB experiments demonstrate the difference in performance between Kira SE and the C version
for large dataset processing. Finally, we show some interesting results when running the C
version on the Edison supercomputer that is deployed at National Energy Research Scientific Computing Center (NERSC).

In all experiments using Amazon's EC2 service, we use the m2.4xlarge instance.
This instance type has eight cores (each running at 2.4GHz), 68~GB RAM, two hard disk drives (HDD), and Gigabit Ethernet. 
We chose a HDD based configuration as opposed to Amazon's newer solid state drive (SSD) backed instances to give a fair comparison
against the Edison supercomputer, which is backed by HDDs. 
Software configurations are described in the following experiments.

\subsection{Single Machine Performance}
\label{sec:Performance-scaleup}

The purpose of the scale-up experiments is to understand Spark's overhead by running
Kira SE on a single node. Slowdown is caused mainly by Java Virtual Machine (JVM) overhead. 
We also wanted to identify the factors
that bound Kira SE's performance. For both Kira SE and the C implementation, we store data
locally in an ext4 file system.

For this experiment, we use a 12GB dataset from the SDSS DR2 survey. The dataset contains
2310 image files and each image is $\sim$6MB.
Figure~\ref{fig:scaleup} shows the relative
performance of Kira SE and the C version for various core counts on a single machine.

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=85mm]{pictures/scaleup}
		\caption{Single-Node Scale-Up Performance Comparison between Kira SE and the C Version (Lower is Better)
		\label{fig:scaleup}}
		\up\up
  	\end{center}
\end{figure}

The C version is bound by local disk performance and shows limited improvement with
an increasing core count. While Kira SE is $7.4\times$ slower than the C implementation
on a single core, Kira SE is within $2.2\times$ when using all eight cores on the node.
The scaling curves indicate that the performance of the approaches begins to converge
as core count increases. At convergence, both of the approaches will be saturating the local
disk bandwidth.

We also profile the C implementation with both warm and cold file system caches. When running with a
cold cache, the job completed in 371 seconds while the job completed in 83 seconds when
running with a warm cache. This indicates that 78\% of job execution time is consumed by
I/O (reading and writing data between local disk and memory). Since the C implementation of
SExtractor is disk bound, we believe that it is representative of a data intensive application. 

\subsection{Scale-Out Performance}
\label{sec:Performance-scaleout}

Next, we wanted to understand the strong scaling performance of both Kira SE and the C
implementation. Although Kira SE has 2.2--7.4$\times$ worse performance when running on
a single machine, we expect that Kira SE will achieve better performance at scale due
to disk locality. We expect that this will allow Kira SE to outperform the C implementation
on large clusters.

We use a 65GB dataset from the SDSS DR2 survey that comprises 11,150 image files.
Kira SE was configured to use HDFS as a storage system, while the C version used GlusterFS. 
Both HDFS and GlusterFS are configured with a replication factor of two.

Figure~\ref{fig:scaleout} shows the performance of Kira SE and the C version across
multiple compute nodes in log scale. Kira SE is 2.7x slower as the C version
on eight cores. However, the gap between the two implementations decreases as we scale up. 
On 256 cores and 512 cores, Kira SE is respectively 5.6\% and 22.4\% faster than the C version.
Across all scales, Kira SE achieves near linear speedup.

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=85mm]{pictures/scaleout}
		\caption{Scale-Out Performance Comparison between Kira SE and the C Version in Logarithmic Scale (Lower is Better)
		\label{fig:scaleout}}
		\up\up
  	\end{center}
\end{figure}

The fundamental driver of Kira SE's linear scalability is it's consistent local disk
hit ratio, which is the ratio between the number of tasks that access the input file 
on local disk and total number of tasks. Spark and HDFS optimize for data locality during scheduling and achieve a
hit ration above 98\%, as shown in Figure~\ref{fig:locality}. In contrast, the
C implementation's locality hit ratio decreases linearly with the log of the cluster
size increases.

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=85mm]{pictures/locality}
		\caption{Locality Hit Ratio (Higher is Better)
		\label{fig:locality}}
		\up\up
  	\end{center}
\end{figure}

In general, a shared file system can be configured in many ways to achieve better
availability, performance, and resilience. To understand the impact of the shared
file system configuration, we compare the performance of Kira SE (time-to-solution)
against four configurations of GlusterFS. The four configurations are
\emph{distributed}, \emph{replicated}, \emph{striped}, and \emph{striped replicated}. 
Table~\ref{tb:gluster-conf} explains the data layout of each configuration.
When possible, we set the replication and striping factors to two.
GlusterFS manages metadata in a distributed manner by spreading metadata across
all available nodes with a hash function. This allows the clients to deterministically
know the location of the metadata of a given file name in the cluster.

\begin{table}[h]
  \begin{center}
  \caption{GlusterFS Configuration Modes and Data Layout}
    \begin{small}
    \begin{tabular}{ | p{1.65cm} | p{6cm} |}
    \hline
    Configuration & Data Layout \\ \hline \hline
    distributed & files are distributed to all nodes without replication  \\ \hline
    replicated & files are distributed to all nodes with a number of replicas specified by the user \\ \hline  
    striped & files are partitioned into a pre-defined number of stripes then distributed to all nodes without replication \\ \hline
    striped replicated & files are partitioned into a pre-defined number of stripes and the stripes are distributed to all nodes with a number of replicas specified by the user \\ \hline
    \end{tabular}
    \end{small}   
  \label{tb:gluster-conf}     	
  \end{center}
\end{table}

We evaluate these configurations using the same dataset as the scale-out experiment. We
select 128 cores and 256 cores as the target scale since it is the transition point in
Figure~\ref{fig:scaleout} where Kira SE begins to outperform the C version. As stated
previously, the C version performs a two-step process. The first step collects and
partitions all filepaths. We refer to this step as metadata overhead. The processing
step occurs next, and is where each node processes its own partition.
Figure~\ref{fig:allgluster} shows the performance of Kira SE and the C version with
profiled metadata overhead.  

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=85mm]{pictures/allgluster}
		\caption{Kira SE Performance Compared to All GlusterFS Configurations (Lower is Better)
		\label{fig:allgluster}}
		\up\up
  	\end{center}
\end{figure}

The C version running in the \emph{distributed} mode outperforms Kira SE at both scales. 
However, the \emph{distributed} mode is not practical in a cloud environment since
it has no replication or any other resilience mechanism. Replicating or striping
files introduces extra metadata overhead when we compare the \emph{replicated} mode
to the \emph{distributed} mode. The metadata overhead for each configuration increases
with large scales due to the cost of managing distributed metadata.

Another observation is that striping will further slow down metadata processing, whereas the
processing part takes less time than the \emph{distributed} mode for both scales due
to the doubled probability of accessing half file (with the striping factor of two) in local disk.
Since the input files are $\sim$6MB each, and are always processed by a single task, the
\emph{replicated} mode should be preferred to the \emph{striped replicated} mode.

On 256 cores, Kira SE outperforms all GlusterFS configurations except for the \emph{distributed}
mode. Kira SE delivers comparable performance to this mode (18\% slower). In our
experiments with the 1TB dataset in Section~\ref{sec:1TB-EC2}, Kira SE outperforms
the \emph{distributed} mode.

\subsection{1TB Dataset Performance}
\label{sec:Performance-1TB}

We select a 1TB dataset from the SDSS DR7 survey, which is comprised of 176,938 image files. 
With the performance measurements, we seek to answer the following questions: 

\begin{itemize}
\item Can Kira scale to process a 1TB dataset?
\item What is the relative performance of Kira compared to the C version on EC2 resources?
\item Can supercomputers (a different architecture with storage connected to computing
resources via a high-speed network) outperform Kira?
\end{itemize}

\subsubsection{Cloud}
\label{sec:1TB-EC2}

We configure GlusterFS in \emph{replicated} and \emph{distributed} modes and compare Kira
SE's performance against the C implementation. Kira SE runs $1.1\times$ and $1.3\times$ faster than the C version running on top of
GlusterFS configured in \emph{distributed} mode on 256 cores and 512 cores respectively. 
Using the more practical \emph{replicated} configuration of GlusterFS, Kira SE
is $3.2\times$ and $3.7\times$ faster. A detailed breakdown of the performance numbers is shown in Figure~\ref{fig:1tb-ec2}.
The C version in \emph{distributed} is slower due to the node starvation that is introduced by our parallel solution in the batch mode.
The C version in \emph{replicated} slows down 2.8x than that in \emph{distributed} mode because the directory metadata query
is dramatically slowdown (13.5x), and the additional replica for each output file and associated metadata update introduces a slowdown
of 2.2x.

for several possible reasons: One of them is that the C version is executed in parallel in a batch mode 
where node starvation can occur at the end of a run. The other is that the metadata overhead of processing 176,938 files in a 
single directory introduces significant overhead.

\begin{figure}[t]
	\begin{center}
		\includegraphics[width=85mm]{pictures/1TB-EC2}
		\caption{Kira SE Performance with 1TB Input Compared to the C Version Running on GlusterFS on EC2 (Lower is Better)
		\label{fig:1tb-ec2}}
		\up\up
  	\end{center}
\end{figure}

Comparing to the experiment with the 65GB dataset in Section~\ref{sec:Performance-scaleout},
Kira SE processes $15.9\times$ more data in $12.5\times$ more time. Removing the Spark
startup time, we can see that Kira SE scales linearly in relation to the data size (though
the number shows super linear scalability).

The overall throughput of Kira SE is 791MB/second, which is $2.4\times$ greater than necessary
to support the upcoming Large Synoptic Survey Telescope (LSST), as discussed in
Section~\ref{sec:Background-EngReq}. This high throughput enables real-time image processing.

\subsubsection{Supercomputer}

Many astronomers have access to supercomputers and believe that
supercomputers outperform commodity clusters for data-intensive applications.
To examine this belief, we compare Kira SE on the Amazon cloud versus the performance of
the C version running on the NERSC Edison supercomputer, a Cray XC 30 System. We use
the Lustre file system which provides a peak throughput of 48GB/s. Each compute
node of Edison is equipped with a 24-core Ivy Bridge processor, with a 2.4GHz clock rate.
This is comparable to the CPU speed of the Amazon EC2 m2.4xlarge instance (eight vCPUs of
Intel Xeon E5-2665, each running at 2.4GHz). The experiments on Edison run on 21
nodes (a total of 504 cores) in the Cluster Compatibility Mode~(CCM)
while Kira SE uses 64 nodes (512 cores) on EC2. The Cray CCM provides a 
standard Linux cluster environment with services such as ssh, rsh, nscd, and ldap which 
are not supported in Cray's native mode.
Figure~\ref{fig:1tb-edison} shows the measurements.

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=85mm]{pictures/1TB-edison}
		\caption{Kira SE Performance with 1TB Input Compared to the C Version Running on NERSC Edison Supercomputer (Lower is Better)
		\label{fig:1tb-edison}}
		\up
  	\end{center}
\end{figure}

During the experiments, we see the C version performance varies significantly. These results
clearly fall into two classes. The first class has an average time-to-solution of 937.8 seconds
with a standard variation of 69.5 seconds. The second class has an average time-to-solution 
of 1983.2 seconds with a variation of 30.9 seconds. A further analysis shows that we are only 
using 0.4\% of the computing resources of the Edison machine. In the first class, the sustained
I/O bandwidth is 1.0 GB/s that is 2.1\% of the I/O bandwidth available on the file system.
While in the second class, the sustained I/O bandwidth is down to 0.5GB/s. Given the computing
resources are completely isolated between jobs, it is the I/O network resource that results
in the varying performance. In Figure~\ref{fig:1tb-edison}, we use the idle mode to refer to the 
situation where the I/O network is idle and the busy mode to refer to the situation where the
I/O network is saturated. In general, Kira SE performs competitively as that of the C version
on Edison supercomputers. 


\section{Related Work}
\label{sec:Related}

Many systems have tackled the problem of executing single process programs in parallel
across large compute clusters. This includes workflow systems such as HTCondor,
and ad hoc Hadoop and MPI based approaches.

In a workflow system, programmers can easily connect serial/parallel programs by specifying
dependencies between tasks and files. These systems do not require any modifications to
the original code base. Workflow systems provide a flexible data management and task execution
scheme that can be applied to a broad range of applications, but at the cost of programming
flexibility.

Researchers have used the Hadoop MapReduce~\cite{HADOOP} system to parallelize
tasks using a map-reduce data model. A variety of scientific applications have been parallelized
using Hadoop such as CloudBLAST~\cite{matsunaga08}. 
Although Hadoop exposes many convenient abstractions, it is difficult to express the application 
with the restrictive map-reduce API~\cite{dewitt08} and Hadoop's disk based model makes 
iterative/pipelined tasks expensive.

MPI has also been used to parallelize a diverse range of workloads. There
are MPI-based parallel implementations of astronomy image
mosaicing applications (Montage~\cite{jacob09}) and
sequence alignment and search toolkits (mpiBLAST~\cite{lin08}) applications. As an
execution system, MPI has two significant drawbacks. First, to implement a many-task
application on top of MPI, a user must develop a custom C wrapper for the application
and a custom message-passing approach for communicating between nodes. In practice, the
communication stages are critical for performance, which means that the
dataflow management scheme must be tailored to the application and hand tuned. Additionally,
MPI does not provide fault tolerance, which is problematic when running a long lived
application across many (possibly) unreliable nodes.

Traditionally, distributed workflow systems are run on top of a shared file system. 
Shared file systems (e.g. Lustre~\cite{donovan03}, and
GlusterFS~\cite{davies13}) are commonly used because they are compatible with the POSIX
standard and offer a shared namespace across all nodes. However, shared file systems
do not expose file locality to workflow systems, thus making suboptimal use of local
disks on the compute nodes when possible. Most tools in the Hadoop ecosystem use
HDFS~\cite{shvachko10}). HDFS  provides a shared namespace, but are not POSIX
compliant. Unlike traditional server-based shared file systems, HDFS uses
the disks on the compute nodes which enables data locality on filesystem access.

\section{Future Work}
\label{sec:Future}

The current Kira SE implementation is CPU bound. We plan to keep improving Kira SE's performance
on a single node until it is disk bound. This will enable Kira SE to make optimal use of
solid state drives~(SSDs) or in-memory file system that provide high I/O throughput.

Although Kira is currently available as an alpha release (\url{https://github.com/BIDS/Kira}), we are planning an official Kira 0.1 release
that implements the source extraction application in Fall 2015.
We plan to migrate to PySpark, which will enable better integration with other
Python-based astronomy tools. Additionally, we plan to provide several further enhancements to
the source extraction API.

Beyond the 0.1 release, we plan to integrate Kira with more astronomy image processing
programs, such as image reprojection and image co-addition. This will allow Kira to be
used as an end-to-end astronomy image analysis pipeline. We use this end-to-end pipeline
to continue evaluating the use of Spark as a conduit for many-task dataflow pipelines by
comparing against the equivalent C implementation. With this system, we will try to determine
which data intensive scientific applications execute most efficiently using ``big data''
software architectures on commodity clusters rather than using HPC software methods on supercomputers.
From this, we hope to obtain insights that can drive the development of novel computing
infrastructure for many-task scientific applications.

\section{Conclusion}
\label{sec:Conclusion}

In this paper, we investigated the idea of leveraging the modern big data platform for many-task
scientific applications. Specifically, we built Kira (\url{https://github.com/BIDS/Kira}), a flexible, scalable,
and performant astronomy image processing toolkit using Apache Spark running on Amazon EC2 Cloud. We also presented
the real world Kira Source Extractor application, and use this application to study the programming
flexibility, dataflow richness, scheduling capacity and performance of the surrounding ecosystem.

The Kira SE application demonstrates linear scalability with both increasing cluster and data
size. Due to its superior data locality, our Spark-based implementation delivers better performance than the equivalent C 
implementation running on GlusterFS. Specifically, Kira SE processes the 1TB SSDS DR7 dataset (176,938 tasks)
$3.7\times$ faster than C over GlusterFS when running on a cluster of 64 m2.4xlarge Amazon
EC2 instances. Kira SE also has comparable performance (between 18.5\% slower and 75.1\% faster)
to that of the C version running on the NERSC Edison supercomputer.  

We also demonstrated that Big Data platforms such as Apache Spark can integrate existing code base at the library level. 
This allows users to reuse the source code to build new analysis pipelines.
The flexible interface, rich dataflow support, task scheduling capacity, locality optimization, and built-in support for fault tolerance make Spark a 
strong candidate for many-task scientific applications. 
We believe this is important, because leveraging such platforms would enable scientists to benefit from the rapid pace of innovation and large range of systems and technologies that are being driven by wide-spread interest in Big Data analytics.

%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}

This research is supported in part by NSF CISE Expeditions Award CCF-1139158, DOE Award SN10040 DE-SC0012463, and DARPA XData Award FA8750-12-2-0331, and gifts from Amazon Web Services, Google, IBM, SAP, The Thomas and Stacey Siebel Foundation, Adatao, Adobe, Apple, Inc., Blue Goji, Bosch, C3Energy, Cisco, Cray, Cloudera, EMC2, Ericsson, Facebook, Guavus, HP, Huawei, Informatica, Intel, Microsoft, NetApp, Pivotal, Samsung, Schlumberger, Splunk, Virdata and VMware. Author Frank Austin Nothaft is supported by a National Science Foundation Graduate Research Fellowship.

This research is also supported in part by the Gordon and Betty Moore
Foundation and the Alfred P. Sloan Foundation together through the
Moore-Sloan Data Science Environment program.
%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{Kira} % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns



%\balancecolumns

% That's all folks!
\end{document}
